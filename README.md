# ğŸ  House Price Prediction â€“ King County

Predicting house prices using **supervised regression models** on the King County dataset (Kaggle).

## ğŸ“˜ Dataset

**Source:** [King County House Prices Dataset â€“ Kaggle](https://www.kaggle.com/datasets/minasameh55/king-country-houses-aa)

This dataset includes homes sold in King County (Seattle area) with detail features such as:
- Bedrooms, bathrooms, living area 
- Floors, condition, grade, and location coordinates (latitude, longitude)
- Year built and renovated
- Sale price (`price`) â€” the **target variable**

## ğŸš€ Project Highlights

- Built a **full ML workflow**: data cleaning â†’ feature engineering â†’ model training â†’ evaluation â†’ hyperparameter tuning.
- Compared **multiple regression models**:
  - Linear Regression, KNN, Decision Tree, AdaBoost, Gradient Boosting, XGBoost, Random Forest
- Applied advanced techniques:
  - Price capping & log-transformation
  - Feature scaling (Standardization)
  - Hyperparameter tuning with RandomizedSearchCV
- **Best-performing models**: Random Forest vs XGBoost

---

## ğŸ§­ Project Workflow Overview

### ğŸ”¹ **Part I â€“ Data Preprocessing**
This notebook focuses on preparing and understanding the dataset before modeling.

1. **Data Cleaning & Exploration**
   - Loaded and explored the dataset.
   - Checked for duplicates, missing values, and data inconsistencies.
   - Visualized distributions and correlations to understand feature relationships.

2. **Feature Selection & Engineering**
   - Identified important predictors using correlation analysis and feature importance.
   - Created new engineered features and dropped irrelevant ones.
   - Prepared clean and processed data for modeling.

---

### ğŸ”¹ **Part II â€“ Modelling and Evaluation **
This notebook builds, evaluates, and improves various regression models.

#### **1. Baseline Model Training and Evaluation**
Trained and compared several supervised regression models:
- Linear Regression  
- K-Nearest Neighbors (KNN) Regressor  
- Decision Tree Regressor  
- AdaBoost Regressor  
- Gradient Boosting Regressor  
- XGBoost Regressor  
- Random Forest Regressor  

**Evaluation Metrics:**
- Mean Squared Error (MSE)  
- R-squared (RÂ²)

#### **2. Feature Engineering and Model Enhancement**
To improve performance and handle skewed data:
1. **Feature Engineering â€“ Target Variable (`price`)**
   - Applied **price capping over Q3** to handle extreme outliers.
   - Applied **log-transformation** on price for normalization.
2. **Feature Scaling**
   - Used **StandardScaler** for standardization of numeric features.
3. **Hyperparameter Tuning**
   - Applied **RandomizedSearchCV** to optimize model parameters for ensemble methods.

#### **3. Best Model Comparison**
Compared the two top-performing ensemble models â€” **Random Forest** and **XGBoost** â€” using:
- RÂ² and MSE (train vs. test)
- Scatter plots of Actual vs. Predicted values
- Overfitting analysis (trainâ€“test performance gap)
- Feature importance visualization

---


## ğŸ“Š Key Results

| Model | Train RÂ² | Test RÂ² | Train MSE | Test MSE |
|-------|----------|---------|-----------|----------|
| Random Forest | 0.97 | 0.88 | 0.01 | 0.03 |
| XGBoost | 0.90 | 0.88 | 0.03 | 0.03 |

âœ… XGBoost generalizes better  
âš ï¸ Random Forest slightly overfits

---

## ğŸ”¹ Tools & Skills Demonstrated

- Python, Jupyter Notebook  
- pandas, numpy, scikit-learn, xgboost  
- Data preprocessing, feature engineering, regression modeling  
- Hyperparameter tuning, model evaluation, ensemble methods  
- Visualization: matplotlib, seaborn

---

## ğŸ“ˆ Visualizations

- Actual vs Predicted prices  
- Model performance comparison (RÂ², MSE)  
- Overfitting analysis  
- Feature importance

---

## ğŸ“‚ Project Files

- **Part I:** Data cleaning & feature engineering â†’ `Part_I_Data_Preprocessing.ipynb`  
- **Part II:** Modeling & evaluation â†’ `Part_II_Modeling_and_Evaluation.ipynb`  
- **Presentation:** Key insights â†’ `presentation.pdf`  

---

## ğŸ‘¨â€ğŸ’» Author

**Hyejeong Hayley Lee**  
ğŸ“§ hyejeong0617@gmail.com
Github: https://github.com/hyejeong0617


